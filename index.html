<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
		<!-- Replace the content tag with appropriate information -->
		<meta name="description"
			content="OVBench: How Far is Your Video-LLMs from Real-World Online Video Understanding?">
		<meta property="og:title" content="OVBench" />
		<meta property="og:description"
			content="OVBench: How Far is Your Video-LLMs from Real-World Online Video Understanding?" />
		<meta property="og:url" content="https://joeleelyf.github.io/OVBench/" />
		<!-- Keywords for your paper to be indexed by-->
		<meta name="keywords" content="online;video;benchmark">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<title>OVBench</title>

		<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
		<link rel="icon" type="image/x-icon" href="static/images/logo.png">
		<link rel="stylesheet" href="static/css/bulma.min.css">
		<link rel="stylesheet" href="static/css/bulma-carousel.min.css">
		<link rel="stylesheet" href="static/css/bulma-slider.min.css">
		<link rel="stylesheet" href="static/css/fontawesome.all.min.css">
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
		<link rel="stylesheet" href="static/css/index.css">

		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
		<script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
		<script defer src="static/js/fontawesome.all.min.js"></script>
		<script src="static/js/bulma-carousel.min.js"></script>
		<script src="static/js/bulma-slider.min.js"></script>
		<script src="static/js/index.js"></script>
	</head>
	<body>

		<section class="hero">
			<div class="hero-body">
				<div class="container is-max-desktop">
					<div class="columns is-centered">
						<div class="column has-text-centered">
							<h1 class="title is-1 publication-title"><b style="color: #0088cc;">OV</b>Bench:</h1>
							<h2 class="subtitle is-3 publication-subtitle">
								How Far is Your Video-LLMs from Real-World <b style="color: #0088cc;">O</b>nline <b style="color: #0088cc;">V</b>ideo Understanding?
							</h2>
							<div class="is-size-5 publication-authors">
								<!-- Paper authors -->
								<span class="author-block">
									<a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First
										Author</a><sup>*</sup>,</span>
								<span class="author-block">
									<a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second
										Author</a><sup>*</sup>,</span>
								<span class="author-block">
									<a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
								</span>
							</div>

							<div class="is-size-5 publication-authors">
								<span class="author-block">Shanghai AI Laboratory</span>
								<span class="eql-cntrb"><small><br><sup>*</sup>Indicates
										Equal Contribution</small></span>
							</div>

							<div class="column has-text-centered">
								<div class="publication-links">
									<!-- Arxiv PDF link -->
									<span class="link-block">
										<a href="static/pdfs/paper.pdf" target="_blank"
											class="external-link button is-normal is-rounded is-dark">
											<span class="icon">
												<i class="fas fa-file-pdf"></i>
											</span>
											<span>Paper</span>
										</a>
									</span>

									<!-- Supplementary PDF link -->
									<span class="link-block">
										<a href="static/pdfs/supplementary.pdf" target="_blank"
											class="external-link button is-normal is-rounded is-dark">
											<span class="icon">
												<i class="fas fa-file-pdf"></i>
											</span>
											<span>Supplementary</span>
										</a>
									</span>

									<!-- Github link -->
									<span class="link-block">
										<a href="https://github.com/JoeLeelyf/OVBench" target="_blank"
											class="external-link button is-normal is-rounded is-dark">
											<span class="icon">
												<i class="fab fa-github"></i>
											</span>
											<span>Code</span>
										</a>
									</span>

									<!-- ArXiv abstract Link -->
									<span class="link-block">
										<a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
											class="external-link button is-normal is-rounded is-dark">
											<span class="icon">
												<i class="ai ai-arxiv"></i>
											</span>
											<span>arXiv</span>
										</a>
									</span>

									<!-- Huggingface Link -->
									<span class="link-block">
										<a href="https://huggingface.co/datasets/JoeLeelyf/OVBench" target="_blank"
											class="external-link button is-normal is-rounded is-dark">
											<span class="icon">
												<p style="font-size:18px">
													ðŸ¤—
												</p>
											</span>
											<span>Dataset</span>
										</a>
									</span>
								</div>
							</div>
						</div>
					</div>
				</div>
			</div>
		</section>

		<section class="section hero is-centered">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="columns is-four-fifths">
						<font size="6">
							<br>ðŸ”¥<b>Release Note</b>
						</font>
					</div>
				</div>
				<div class="columns is-centered has-text-left">
					<div class="columns is-four-fifths">

						<font size="4">
							<br> <b style="color:#E40304">[To-Be-Done]</b> The release of <b>Training Set</b>
							<br> <b style="color:#7DDA58">[2024.12.05]</b> The <b>Paper</b>, <b>Eval Code</b>, and
							<b>Dataset</b> are released!
						</font>
					</div>
				</div>
			</div>
		</section>

		<!-- Paper abstract -->
		<section class="section hero is-centered">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-four-fifths">
						<h2 class="title is-3">Abstract</h2>
						<div class="content has-text-justified">
							<p>
								Integrating past information and adapting to continuous video input are pivotal for
								human-level video understanding.
								Current benchmarks, however, focus on coarse-grained, video-level question-answering in
								offline settings,
								limiting real-time processing and adaptability for practical applications.
								To this end, we introduce <b>OVBench</b> <b>O</b>nline-<b>V</b>ideo-<b>Bench</b>mark),
								which assesses online video understanding through three modes:
								(1) <b>Backward Tracing</b>, (2) <b>Real-Time Visual Perception</b>, and (3) <b>Forward
									Active Responding</b>.
								OVBench consists of 12 tasks, comprising about 2,800 meta-annotations with fine-grained,
								event-level timestamps paired with 858 videos across 10 domains,
								encompassing egocentric activities, virtual gaming worlds, and cinematic scenes.
								To minimize bias, we employ automated generation pipelines and human annotation for
								meticulous curation.
								We design an effective problem generation and evaluation pipeline based on these
								high-quality samples and densely query Video-LLMs across the video streaming timeline.
								Extensive evaluations of nine Video-LLMs reveal that despite rapid advancements and
								improving performance on traditional benchmarks, existing models struggle with online
								video understanding.
								Our comprehensive evaluation reveals that the best-performing models still have a
								significant gap compared to human agents in online video understanding.
								We anticipate that OVBench will guide the development of Video-LLMs towards practical
								real-world applications and inspire future research in online video understanding.
							</p>
						</div>
					</div>
				</div>
			</div>
		</section>
		<!-- End paper abstract -->

		<!-- Taxnomy -->
		<section class="section hero is-light">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<h2 class="title is-3">Task Taxnomy of OVBench</h2>
					</div>
				</div>
			</div>
		</section>
		<section class="section hero is-centered">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-four-fifths">
						<div class="content has-text-justified">
							<br>Online video understanding aims to equip real-world, 
							always-on agents with the ability to receive and process 
							video inputs continuously, which closely mimics the human visual perception process. We categorize online video 
							understanding into three distinct problem-solving modes: 
							<br>(1) <b>Backward Tracing</b>, (2) <b>Real-Time Visual Perception</b>, and (3) <b>Forward Active Responding</b>.
							<br> Tasks of different modes are as follows:
							<ol>
								<li> <b>Backward Tracing</b>
								<li> <b>Real-Time Visual Perception</b>
								<li> <b>Forward Active Responding</b>
							</ol>
						</div>
					</div>
				</div>
			</div>
		</section>
		<!-- Taxnomy -->

		<!-- Comparision -->
		<section class="section hero is-light">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<h2 class="title is-3">Comparision of OVBench and Other VideoQA benchmarks</h2>
					</div>
				</div>
			</div>
		</section>
		<section class="section hero is-centered">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-four-fifths">
						The current evaluation of video-LLMs
					</div>
				</div>
			</div>
		</section>

		<!-- Comparision -->

		<!-- Leader Board -->
		<section class="section hero is-light">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<h2 class="title is-3">OVBench Leaderboard</h2>
					</div>
				</div>
			</div>
		</section>
		<section class="section hero is-centered">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-four-fifths">
						<br>The performance of latest mainstream Video-LLMs on our benchmark, including two 2
						close-sourced models and 6 representative open-source models.
						<br>The best results of each category are highlighted in <b>bold</b>
					</div>
				</div>
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<img src="static/images/leaderboard.png">
					</div>
				</div>
			</div>
		</section>
		<!-- Leader Board -->

		<!-- More Dataset Examples -->
		<section class="section hero is-light">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<h2 class="title is-3">More Dataset Examples</h2>
					</div>
				</div>
			</div>
		</section>
		<section class="hero is-small is-full">
		  <div class="hero-body">
		    <div class="container">
		
		      <iframe  src="static/examples/benchmark_examples_supplementary.pdf" width="100%" height="550">
		          </iframe>
		        
		      </div>
		    </div>
		  </section>
		<!-- More Dataset Examples -->

		<!--BibTex citation -->
		<section class="section" id="BibTeX">
			<div class="container is-max-desktop content">
				<h2 class="title">BibTeX</h2>
				<pre><code>BibTex Code Here</code></pre>
			</div>
		</section>
		<!--End BibTex citation -->

		<!-- Statcounter tracking code -->

		<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

		<!-- End of Statcounter Code -->

	</body>
</html>